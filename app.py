import asyncio
import base64
import shutil
from concurrent.futures import ThreadPoolExecutor
from datetime import datetime
import flask
import numpy as np
import portalocker
import boto3
import json

import sounddevice
import speech_recognition as sr
from amazon_transcribe.client import TranscribeStreamingClient
from amazon_transcribe.handlers import TranscriptResultStreamHandler
from amazon_transcribe.model import TranscriptResultStream, TranscriptEvent
from flask import Flask, render_template, Response, send_from_directory, request, jsonify, send_file
from flask_socketio import SocketIO
import threading
import time
from selenium.webdriver.chrome.service import Service
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity
from werkzeug.utils import secure_filename
from api_request_schema import api_request_list
import os
import markdown
from selenium import webdriver
from selenium.common.exceptions import TimeoutException
import sys
import uuid
from pathlib import Path
import fitz # pip install PyMuPDF
import re
from flask_cors import CORS # pip install flask-cors
import io
from langchain.document_loaders import TextLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.embeddings.fastembed import FastEmbedEmbeddings
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_experimental.text_splitter import SemanticChunker
from typing import List, Tuple, Optional
from transformers import AutoTokenizer, AutoModel
import torch
import torch.nn.functional as F
from transformers import AutoTokenizer, AutoModel

# åˆå§‹åŒ–
app = Flask(__name__)
CORS(app)  # â¬…ï¸ å…è®¸ JS è·¨åŸŸè¯·æ±‚ MP3 æµ
app.config.update(
    UPLOAD_FOLDER = 'uploads',          # ä¸Šä¼ ç›®å½•
    ALLOWED_EXTENSIONS = {'png', 'jpeg', 'gif', 'webp', 'pdf'},  # å…è®¸çš„æ–‡ä»¶ç±»å‹
    MAX_CONTENT_LENGTH = 5 * 1024 * 1024  # é™åˆ¶5MBå¤§å°
)
socketio = SocketIO(app, cors_allowed_origins="*")
bedrock_runtime = boto3.client("bedrock-runtime", region_name="us-east-1")# åˆå§‹åŒ– AWS Bedrock å®¢æˆ·ç«¯

# å¯è°ƒè®¾ç½®å‚æ•°
class Config:
    # è¶…å‚æ•°
    refresh_rag_documents = True
    voiceIndex = 0 # è¯­éŸ³ä»£ç ï¼š0:ä¸­æ–‡ï¼Œ1:è‹±æ–‡ï¼Œ2:æ—¥æ–‡ï¼Œ3:æ³•è¯­
    audio = False  # é»˜è®¤æ˜¯å¦è¯­éŸ³è¾“å‡º
    temperature = 0.1  # é»˜è®¤æ¸©åº¦
    embedding_model_name_index = 0  # åµŒå…¥æ¨¡å‹åç§°
    use_RAG = False
    model_index = 0
    model_id_list = ['anthropic.claude-3-5-sonnet-20240620-v1:0','anthropic.claude-3-sonnet-20240229-v1:0','meta.llama3-70b-instruct-v1']
    model_id = os.getenv('MODEL_ID', model_id_list[model_index])
    api_request = api_request_list[model_id]

    # ä¸ä¿®æ”¹å‚æ•°
    embedding_model_names = ['BAAI/bge-reranker-base', 'BAAI/bge-large-en-v1.5', 'BAAI/bge-large-zh-v1.5', 'BAAI/bge-m3', 'Alibaba-NLP/gte-multilingual-base', 'ibm-granite/granite-embedding-278m-multilingual']
    SETTINGS_FILE = 'static/settings/saved_settings.json'
    aws_region = os.getenv('AWS_REGION', 'us-east-1')
    voiceList = ["zh-CN", "en-US", "ja-JP", "fr-FR"]
    voiceLanguageList = ['cmn-CN', 'en-US', 'ja-JP', 'fr-FR']
    voiceNameList = ['Zhiyu', 'Ivy', 'Takumi', 'Remi']
    config = {
        'log_level': 'none',
        'region': aws_region,
        'polly': {
            'Engine': 'neural',
            'LanguageCode': voiceLanguageList[voiceIndex],
            'VoiceId': voiceNameList[voiceIndex],
            'OutputFormat': 'mp3',
        },
        'bedrock': {
            'api_request': api_request
        }
    }
    MEMORY_FILE = 'global_memory/memory.json'
    SERIES_FILE = 'chat_series/chat_series.json'

    @staticmethod
    def update_config_voice():
        Config.config['polly']['LanguageCode'] = Config.voiceLanguageList[Config.voiceIndex]
        Config.config['polly']['VoiceId'] = Config.voiceNameList[Config.voiceIndex]

# å…¨å±€è·¯ç”±å¤„ç†
class URL:
    # åˆ›å»ºé¦–é¡µè·¯ç”±
    @staticmethod
    @app.route("/")
    def index():
        return render_template("index.html")

    # åˆ›å»ºprismåº“çš„è·¯ç”±
    @staticmethod
    @app.route('/prism/<filename>')
    def prism(filename):
        return send_from_directory('prism', filename)  # ç›´æ¥ä»prismæ–‡ä»¶å¤¹å‘é€æ–‡ä»¶

# æ¨¡å‹è¾“å‡ºå¤„ç†
class Bedrock:
    interrupted = False
    audio_cache = dict()  # å­˜å‚¨éŸ³é¢‘æ•°æ®

    # è¾“å…¥
    @staticmethod
    def generate_response(user_text, user_text_with_prompt):
        print(Config.audio)
        ai_response_html = ''

        print(Config.audio)
        socketio.emit("begin_generate_response")
        ai_response = ''
        raw_ai_response = ''
        ai_response_char_iter = Bedrock.char(user_text_with_prompt)
        for char in ai_response_char_iter:

            if Bedrock.interrupted:
                break
            ai_response += char
            raw_ai_response += char
            if char == '\\':
                ai_response += char
            ai_response_html = markdown.markdown(ai_response, extensions=['fenced_code', 'extra'])
            socketio.emit("refresh_ai_response", ai_response_html)
        if Config.audio:
            UserInput.history.append('ç”¨æˆ·ï¼ˆè¯­éŸ³ï¼‰ï¼š' + user_text)
        else:
            UserInput.history.append('ç”¨æˆ·ï¼ˆæ–‡å­—ï¼‰ï¼š' + user_text)
        if Bedrock.interrupted:
            for _ in ai_response_char_iter:
                pass
            Bedrock.interrupted = False
            if Config.audio:
                UserInput.history.append('AIå›å¤ï¼ˆè¯­éŸ³ï¼Œè¾“å‡ºè¢«æ‰“æ–­ï¼‰ï¼š' + raw_ai_response)
            else:
                UserInput.history.append('AIå›å¤ï¼ˆæ–‡å­—ï¼Œè¾“å‡ºè¢«æ‰“æ–­ï¼‰ï¼š' + raw_ai_response)
        else:
            if Config.audio:
                UserInput.history.append('AIå›å¤ï¼ˆè¯­éŸ³ï¼‰ï¼š' + raw_ai_response)
            else:
                UserInput.history.append('AIå›å¤ï¼ˆæ–‡å­—ï¼‰ï¼š' + raw_ai_response)
        if Config.audio:
            audio_url = Bedrock.bedrock_text_to_speech(raw_ai_response)
            socketio.emit("ai_response_audio",
                          audio_url)
        socketio.emit("ai_response_end", raw_ai_response)

        GlobalHistory.global_history.add_entry(user_text, raw_ai_response)

        ChatSeries.chat_series[ChatSeries.current_chat_index].update_chat_history(
            {"ai": raw_ai_response, "ai_html": ai_response_html})  # æ›´æ–°èŠå¤©è®°å½•

        # å®šä¹‰æ­£åˆ™è¡¨è¾¾å¼ï¼ˆè´ªå©ªæ¨¡å¼åŒ¹é…ï¼‰
        pattern = r'<global-memory>(.*?)</global-memory>'
        memory_matches = re.findall(pattern, raw_ai_response, re.DOTALL)
        if memory_matches:
            print(memory_matches)
            for content in enumerate(memory_matches):
                GlobalMemory.add_memory(content[-1])

        # åœ¨ç”Ÿæˆå®Œå“åº”åï¼Œé‡æ–°å¯ç”¨å¼€å§‹æŒ‰é’®
        socketio.emit("toggle_button", {"button_id": "start-button", "state": False})  # è®©å¼€å§‹æŒ‰é’®å·¥ä½œ

    # è°ƒç”¨ Amazon Bedrock GPT å¤„ç†å¯¹è¯
    @staticmethod
    def char(user_text):
        # noinspection PyShadowingNames
        try:
            body = UserInput.define_body(user_text, MultiModal.decode_images(), MultiModal.read_txt())
            body_json = json.dumps(body)
            response = bedrock_runtime.invoke_model_with_response_stream(
                body=body_json,
                modelId=Config.config['bedrock']['api_request']['modelId'],
                accept=Config.config['bedrock']['api_request']['accept'],
                contentType=Config.config['bedrock']['api_request']['contentType']
            )
            print(Config.config['bedrock']['api_request']['modelId'])

            bedrock_stream = response.get('body')

            model_provider = Config.model_id.split('.')[0]
            text = ''

            if bedrock_stream:
                for event in bedrock_stream:
                    if Bedrock.interrupted:
                        time.sleep(1)
                        return
                    chunk = event.get('chunk')
                    if chunk:
                        text = ''
                        if model_provider == 'meta':
                            chunk_obj = json.loads(chunk.get('bytes').decode())
                            text = chunk_obj['generation']
                        elif model_provider == 'anthropic':
                            if "claude-3" in Config.model_id:
                                chunk_obj = json.loads(chunk.get('bytes').decode())
                                if chunk_obj['type'] == 'message_delta':
                                    if not chunk_obj['delta']['stop_sequence']:
                                        chunk_obj['delta']['stop_sequence'] = "none"
                                    socketio.emit("retrievedInfo",{
                                        "stopReason": chunk_obj['delta']['stop_reason'],
                                        "stopSequence": chunk_obj['delta']['stop_sequence'],
                                        "outputTokens": chunk_obj['usage']['output_tokens']
                                    })

                                    ChatSeries.chat_series[ChatSeries.current_chat_index].update_chat_history(
                                        {"stopReason": chunk_obj['delta']['stop_reason'],
                                        "stopSequence": chunk_obj['delta']['stop_sequence'],
                                        "outputTokens": chunk_obj['usage']['output_tokens']})  # æ›´æ–°èŠå¤©è®°å½•

                                    print(f"\nStop reason: {chunk_obj['delta']['stop_reason']}")
                                    print(f"Stop sequence: {chunk_obj['delta']['stop_sequence']}")
                                    print(f"Output tokens: {chunk_obj['usage']['output_tokens']}")
                                if chunk_obj['type'] == 'content_block_delta':
                                    if chunk_obj['delta']['type'] == 'text_delta':
                                        text = chunk_obj['delta']['text']

                            else:
                                chunk_obj = json.loads(chunk.get('bytes').decode())
                                text = chunk_obj['completion']
                        else:
                            raise NotImplementedError('Unknown model provider.')
                        for char in text:
                                yield char
                        text = ''


        except Exception as e:
            e_text = f"API è°ƒç”¨å‡ºé”™: {e}"
            print(f"API è°ƒç”¨å‡ºé”™: {e}")
            for char in e_text:
                yield char

    # æ·»åŠ éŸ³é¢‘ä¸“å±è·¯ç”±å¤„ç†
    @staticmethod
    @app.route('/stream-audio/<audio_id>')
    def stream_audio(audio_id):
        audio_bytes = Bedrock.audio_cache.get(audio_id)
        if audio_bytes:
            audio_stream = io.BytesIO(audio_bytes)
            response = send_file(
                audio_stream,
                mimetype='audio/mpeg',
                as_attachment=False,
                download_name='speech.mp3'
            )
            response.headers['Access-Control-Allow-Origin'] = '*'
            response.headers['Accept-Ranges'] = 'bytes'
            return response
        else:
            return "Audio expired", 404

    @staticmethod
    # éŸ³é¢‘è¾“å‡ºå‡½æ•°ï¼ˆéŸ³é¢‘ä¼ ç»™htmlï¼‰
    def bedrock_text_to_speech(text):
        # noinspection PyShadowingNames
        try:
            polly = boto3.client('polly', region_name=Config.config['region'])
            polly_response = polly.synthesize_speech(
                Text=text,
                Engine=Config.config['polly']['Engine'],
                LanguageCode=Config.config['polly']['LanguageCode'],
                VoiceId=Config.config['polly']['VoiceId'],
                OutputFormat=Config.config['polly']['OutputFormat'],
            )

            audio_bytes = polly_response['AudioStream'].read()  # ğŸ§  ä¸€æ¬¡æ€§è¯»å®Œ
            audio_id = str(uuid.uuid4())
            Bedrock.audio_cache[audio_id] = audio_bytes  # ğŸ’¾ å­˜å‚¨ä¸ºå­—èŠ‚
            return f'/stream-audio/{audio_id}'  # ğŸ”Š è¿”å›å”¯ä¸€æ ‡è¯†
        except Exception as e:
            print(f"API è°ƒç”¨å‡ºé”™: {e}")
            return "AI å¤„ç†è¯·æ±‚æ—¶å‘ç”Ÿé”™è¯¯"

    # å“åº”ï¼šæ‰“æ–­è¾“å‡º
    @staticmethod
    @socketio.on("interrupt")
    def interrupt():
        Bedrock.interrupted = True
        print('æ‰“æ–­è¾“å…¥')

# è®¾ç½®çª—å£
class Settings:
    temperature = Config.temperature
    audio = Config.audio
    use_RAG = Config.use_RAG
    language = None
    embedding_model_name = Config.embedding_model_name_index
    settings = None
    file_path = Config.SETTINGS_FILE

    @staticmethod
    @socketio.on('start_settings_init')
    def settings_init():
        """
            å®‰å…¨åŠ è½½å’Œè§£æ JSON é…ç½®æ–‡ä»¶
            è¿”å›åŒ…å«æ‰€æœ‰é…ç½®çš„å­—å…¸ï¼Œå¹¶è‡ªåŠ¨è½¬æ¢æ•°æ®ç±»å‹
            """
        try:
            config_file = Path(Settings.file_path)

            # ğŸ›¡ï¸ æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
            if not config_file.is_file():
                raise FileNotFoundError(f"âŒ é…ç½®æ–‡ä»¶ {Settings.file_path} ä¸å­˜åœ¨")

            # ğŸ“– è¯»å–æ–‡ä»¶å†…å®¹
            with open(config_file, 'r', encoding='utf-8') as f:
                config_data = json.load(f)

            Config.temperature = float(config_data.get('temperature', Config.temperature))
            Config.audio = bool(config_data.get('audio', Config.audio))
            Config.voiceIndex = int(config_data.get('language', Config.voiceIndex))
            Config.embedding_model_name_index = int(config_data.get('embedding_model', Config.embedding_model_name_index))
            Config.use_RAG = int(config_data.get('use_RAG', Config.use_RAG))

            print('settings_init:', Config.temperature, Config.audio, Config.voiceIndex, Config.use_RAG, Config.embedding_model_name_index)

            socketio.emit("settings_init", {
                "temperature": Config.temperature,
                "audio": Config.audio,
                "language": Config.voiceIndex,
                "use_RAG": Config.use_RAG,
                "embedding_model": Config.embedding_model_name_index})

        except json.JSONDecodeError:
            print("âŒ JSON æ ¼å¼é”™è¯¯ï¼Œè¯·æ£€æŸ¥é…ç½®æ–‡ä»¶è¯­æ³•")
        except Exception as e:
            print(f"âŒ è¯»å–é…ç½®å‡ºé”™: {str(e)}")

    # å“åº”ï¼šåŠ è½½è®¾ç½®
    @staticmethod
    @socketio.on('update_settings')
    def handle_update_settings(data):
        Settings.temperature = data.get('temperature')
        Settings.audio = bool(data.get('audio_state'))
        Settings.language = data.get('language')
        Settings.use_RAG = data.get('use_RAG')
        Settings.embedding_model_name = data.get('embedding_model')

        # åŠ è½½å½“å‰è®¾ç½®
        Settings.settings = Settings.load_settings()
        if Settings.temperature is not None:
            Settings.settings['temperature'] = Settings.temperature
        if Settings.audio is not None:
            Settings.settings['audio'] = Settings.audio
            Config.audio = Settings.settings['audio']
            print(Config.audio)
        if Settings.language is not None:
            Settings.settings['language'] = Settings.language
            Config.voiceIndex = int(Settings.settings['language'])
            Config.update_config_voice()
        if Settings.use_RAG is not None:
            Settings.settings['use_RAG'] = Settings.use_RAG
            Config.use_RAG = Settings.settings['use_RAG']
        if Settings.embedding_model_name is not None:
            Settings.settings['embedding_model'] = Settings.embedding_model_name
            Config.embedding_model_name_index = int(Settings.settings['embedding_model'])

        # ä¿å­˜è®¾ç½®
        Settings.save_settings(Settings.settings)

        # è¿”å›å“åº”
        socketio.emit('settings-saved', {'success': True, 'settings': Settings.settings})
        print('Updated settings:', Settings.settings)

    @staticmethod
    # ä¿å­˜è®¾ç½®æ–‡ä»¶
    def save_settings(settings):

        with open(Config.SETTINGS_FILE, 'w') as f:
            portalocker.lock(f, portalocker.LOCK_EX)  # æ’ä»–é”
            json.dump(settings, f, indent=4)
            portalocker.lock(f, portalocker.LOCK_UN)  # é‡Šæ”¾é”

    @staticmethod
    # åŠ è½½è®¾ç½®
    def load_settings():
        with open(Config.SETTINGS_FILE, 'r') as f:
            portalocker.lock(f, portalocker.LOCK_SH)  # å…±äº«é”
            settings = json.load(f)
            portalocker.lock(f, portalocker.LOCK_UN)  # é‡Šæ”¾é”
        return settings

# ç”¨äºæš‚åœè¾“å…¥
class UserInputAudio:
    shutdown_executor = False
    executor = None

    @staticmethod
    def set_executor(executor):
        UserInputAudio.executor = executor

    @staticmethod
    def start_shutdown_executor():
        UserInputAudio.shutdown_executor = True

    @staticmethod
    def start_user_input_loop():
        pass

    @staticmethod
    def is_executor_set():
        return UserInputAudio.executor is not None

    @staticmethod
    def is_shutdown_scheduled():
        return UserInputAudio.shutdown_executor

# è¯­éŸ³è½¬æ–‡æœ¬ï¼Œä¼ é€’ç»™Bedrockæ¨¡å‹ç”Ÿæˆå›å¤
class EventHandler(TranscriptResultStreamHandler):
    text = []
    last_time = 0
    sample_count = 0
    max_sample_counter = 4
    history = []

    def __init__(self, transcript_result_stream: TranscriptResultStream, bedrock_wrapper, loop):
        super().__init__(transcript_result_stream)
        self.bedrock_wrapper = bedrock_wrapper
        self.loop = loop

    async def handle_transcript_event(self, transcript_event: TranscriptEvent):
        results = transcript_event.transcript.results
        if UserInputAudio.shutdown_executor:
            return
        if not self.bedrock_wrapper.is_speaking():
            if results:
                for result in results:
                    EventHandler.sample_count = 0
                    if not result.is_partial:
                        for alt in result.alternatives:
                            print(alt.transcript, flush=True, end=' ')
                            EventHandler.text.append(alt.transcript)

            else:
                EventHandler.sample_count += 1
                if EventHandler.sample_count == EventHandler.max_sample_counter:
                    if len(EventHandler.text) != 0:
                        if UserInputAudio.shutdown_executor:
                            return
                        input_text = ''.join(EventHandler.text)
                        executor = ThreadPoolExecutor(max_workers=1)
                        UserInputAudio.set_executor(executor)
                        self.loop.run_in_executor(
                            executor,
                            self.bedrock_wrapper.recognition_loop,
                            input_text
                        )

                    EventHandler.text.clear()
                    EventHandler.sample_count = 0

# ä»éº¦å…‹é£æ•è·éŸ³é¢‘æµï¼Œå‘é€åˆ°Amazon Transcribeè½¬å½•
class MicStream:
    transcribe_streaming = TranscribeStreamingClient(region=Config.config['region'])

    async def mic_stream(self):
        loop = asyncio.get_event_loop()
        input_queue = asyncio.Queue()

        def callback(indata, frame_count, time_info, status):
            loop.call_soon_threadsafe(input_queue.put_nowait, (bytes(indata), status))

        stream = sounddevice.RawInputStream(
            channels=1, samplerate=16000, callback=callback, blocksize=2048 * 2, dtype="int16")
        with stream:
            while True:
                indata, status = await input_queue.get()
                yield indata, status

    async def write_chunks(self, stream):
        async for chunk, status in self.mic_stream():
            await stream.input_stream.send_audio_event(audio_chunk=chunk)
        await stream.input_stream.end_stream()

    async def basic_transcribe(self, loop):
        loop.run_in_executor(ThreadPoolExecutor(max_workers=1), UserInputAudio.start_user_input_loop)
        if UserInputAudio.is_shutdown_scheduled():
            return
        stream = await MicStream.transcribe_streaming.start_stream_transcription(
                language_code=Config.voiceList[Config.voiceIndex],
                media_sample_rate_hz=16000,
                media_encoding="pcm",
        )
        handler = EventHandler(stream.output_stream, UserInput.Speech, loop)
        await asyncio.gather(self.write_chunks(stream), handler.handle_events())

# ç”¨æˆ·è¾“å…¥å¤„ç†
class UserInput:
    history = []

    @staticmethod
    @socketio.on('current_model_index')
    def current_model_index(current_model_index):
        model_index = int(current_model_index)
        Config.model_index = model_index
        Config.model_id = os.getenv('MODEL_ID', Config.model_id_list[model_index])
        Config.config['bedrock']['api_request'] = api_request_list[Config.model_id]
        print(f'è·å–æ›´æ–°æ¨¡å‹ç´¢å¼•ã€‚Config.config[api_request]ï¼š{api_request_list[Config.model_id]}')

    # å‘é€ç»™æ¨¡å‹çš„è¯·æ±‚ä½“
    @staticmethod
    def define_body(text, image_list=None, txt_from_pdf=None):

        model_id = Config.config['bedrock']['api_request']['modelId']
        model_provider = model_id.split('.')[0]
        body = Config.config['bedrock']['api_request']['body']

        if txt_from_pdf:
            print(f"è¯»å–ä¸Šä¼ çš„pdfï¼›{txt_from_pdf}")
            text = f"ç”¨æˆ·ä¸Šä¼ çš„pdfå†…å®¹ï¼š{txt_from_pdf}, ç”¨æˆ·ï¼š" + text

        if model_provider == 'amazon':
            body['inputText'] = text
        elif model_provider == 'meta':
            if 'llama3' in model_id:
                with open('./llama_prompt', 'r', encoding='utf-8') as file:
                    prompt = file.read()
                body['prompt'] = prompt.format(text=text)
            else:
                body['prompt'] = f"<s>[INST] {text}, please output in Chinese. [/INST]"
        elif model_provider == 'anthropic':
            if "claude-3" in model_id:
                s = Settings.load_settings()
                body['temperature'] = float(s['temperature'])
                import claude3_prompts as cp
                if image_list:
                    add = [
                        {
                            "role": "user",
                            "content": [
                                *[
                                    {
                                        "type": "image",
                                        "source": {
                                            "type": "base64",
                                            "media_type": image['type'],
                                            "data": image['data'],
                                        },
                                    }
                                    for image in image_list
                                ],
                                {
                                    "type": "text",
                                    "text": text,
                                },
                            ],
                        }
                    ]
                else:
                    add = [
                        {"role": "user", "content": text}
                    ]
                body['messages'] = cp.promptsMessages + add
                if Config.audio:
                    body['system'] = cp.promptsSystemAudio
                else:
                    body['system'] = cp.promptsSystemText

            else:
                body['prompt'] = f'\n\nHuman: {text}\n\nAssistant:'
        elif model_provider == 'cohere':
            body['prompt'] = text
        elif model_provider == 'mistral':
            body['prompt'] = f"<s>[INST] {text}, please output in Chinese. [/INST]"
        else:
            raise Exception('Unknown model provider.')

        return body

    class Speech:
        speaking = False
        loop = None

        @staticmethod
        def cancel_all_tasks():
            # è·å–æ‰€æœ‰æœªå®Œæˆçš„ä»»åŠ¡
            tasks = [task for task in asyncio.all_tasks(UserInput.Speech.loop) if not task.done()]

            # å–æ¶ˆæ‰€æœ‰ä»»åŠ¡
            for task in tasks:
                task.cancel()

            UserInput.Speech.loop.run_until_complete(asyncio.wait(tasks))

            UserInputAudio.executor.shutdown(wait=False)
            UserInputAudio.shutdown_executor = False
            UserInputAudio.executor = None

            UserInput.Speech.loop.stop()
            UserInput.Speech.loop.close()
            asyncio.set_event_loop(None)

        @staticmethod
        def is_speaking():
            return UserInput.Speech.speaking

        @staticmethod
        # è¯†åˆ«åˆ°éŸ³é¢‘å¹¶ä¼ é€’ç»™æ¨¡å‹ä¸å‰ç«¯
        def recognition_loop(user_text):
            user_text = "<p>" + user_text + "</p>"
            UserInput.Speech.speaking = True
            UserInputAudio.shutdown_executor = True
            # å¤„ç†ç”¨æˆ·è¾“å…¥
            socketio.emit("recognized_text", user_text)  # å‘é€ç”¨æˆ·è¾“å…¥åˆ°å‰ç«¯
            ChatSeries.chat_series[ChatSeries.current_chat_index].update_chat_history({"user": user_text})
            if len(UserInput.history) > 20:
                UserInput.history = UserInput.history[-20:]
            text_history = '\n'.join(UserInput.history)
            memories = GlobalMemory.get_memories()
            if Config.use_RAG:
                retrieved_text = RAG.rag.get_retrieved_text(user_text)
                user_text_with_prompt = f'''<prompt>
ä»¥ä¸‹æ˜¯å¯¹è¯è®°å½•ï¼ˆæœ€åçš„æ˜¯æœ€æ–°ä¸€æ¬¡çš„å¯¹è¯è®°å½•ï¼‰ï¼š<history>
{text_history}
</history>
ä»¥ä¸‹æ˜¯å…¨å±€å¯¹è¯è®°å½•ï¼ˆåŒ…å«äº†æ‰€æœ‰å¯¹è¯ä¸­çš„æœ€è¿‘åæ¡æ¶ˆæ¯ï¼‰ï¼š<global_history>
{GlobalHistory.global_history._history}
</global_history>
è¿™æ˜¯å·²ç»æ·»åŠ çš„å…³äºç”¨æˆ·çš„ä¸€äº›å…¨å±€è®°å¿†ï¼š<global-memory>
{memories}
</global-memory>
ç”¨ç”¨æˆ·çš„é—®é¢˜å¯ä»¥æ£€ç´¢åˆ°ä»¥ä¸‹ç›¸å…³ä¿¡æ¯ï¼š
<rag>
{retrieved_text}
</rag>
ç”¨æˆ·çš„å¯¹è¯æœ€å¯èƒ½ä¸æœ€æ–°çš„ä¸€æ¬¡ï¼ˆå†å²ä¸­æœ€åä¸€ä¸ªï¼‰å¯¹è¯æœ‰å…³ã€‚
è¿™æ˜¯ç”¨æˆ·è¾“å…¥ï¼Œè¯·å›ç­”ï¼š<question>
{user_text}
</question>
ä½ çš„æ‰€æœ‰è¾“å‡ºæ‰€ç”¨çš„è¯­è¨€å¿…é¡»ä¸questionå†…çš„è¯­è¨€ä¸€è‡´ã€‚
</prompt>
'''
            else:
                user_text_with_prompt = f'''<prompt>
ä»¥ä¸‹æ˜¯å¯¹è¯è®°å½•ï¼ˆæœ€åçš„æ˜¯æœ€æ–°ä¸€æ¬¡çš„å¯¹è¯è®°å½•ï¼‰ï¼š<history>
{text_history}
</history>
ä»¥ä¸‹æ˜¯å…¨å±€å¯¹è¯è®°å½•ï¼ˆåŒ…å«äº†æ‰€æœ‰å¯¹è¯ä¸­çš„æœ€è¿‘åæ¡æ¶ˆæ¯ï¼‰ï¼š<global_history>
{GlobalHistory.global_history._history}
</global_history>
è¿™æ˜¯å·²ç»æ·»åŠ çš„å…³äºç”¨æˆ·çš„ä¸€äº›å…¨å±€è®°å¿†ï¼š<global-memory>
{memories}
</global-memory>
ç”¨æˆ·çš„å¯¹è¯æœ€å¯èƒ½ä¸æœ€æ–°çš„ä¸€æ¬¡ï¼ˆå†å²ä¸­æœ€åä¸€ä¸ªï¼‰å¯¹è¯æœ‰å…³ã€‚
è¿™æ˜¯ç”¨æˆ·è¾“å…¥ï¼Œè¯·å›ç­”ï¼š<question>
{user_text}
</question>
ä½ çš„æ‰€æœ‰è¾“å‡ºæ‰€ç”¨çš„è¯­è¨€å¿…é¡»ä¸questionå†…çš„è¯­è¨€ä¸€è‡´ã€‚
</prompt>
'''

            print(user_text_with_prompt)
            Bedrock.generate_response(user_text, user_text_with_prompt)  # ç”ŸæˆéŸ³é¢‘

            # è¯­éŸ³è¯†åˆ«ç»“æŸåï¼Œé‡æ–°å¯ç”¨å¼€å§‹æŒ‰é’®
            socketio.emit("toggle_button", {"button_id": "start-button", "state": False})  # è®©å¼€å§‹æŒ‰é’®å·¥ä½œ

        # å¤„ç†å®¢æˆ·ç«¯è¯­éŸ³è¯†åˆ«è¯·æ±‚
        @staticmethod
        @socketio.on("start_recognition")
        def handle_recognition():
            print('è¯­éŸ³è¯†åˆ«å·²å¼€å§‹')
            if not UserInput.Speech.speaking:
                socketio.emit("toggle_button", {"button_id": "start-button", "state": True})  # ç¦ç”¨å¼€å§‹æŒ‰é’®
                UserInput.Speech.loop = asyncio.new_event_loop()
                asyncio.set_event_loop(UserInput.Speech.loop)
                try:
                    UserInputAudio.shutdown_executor = False
                    UserInput.Speech.loop.run_until_complete(MicStream().basic_transcribe(UserInput.Speech.loop))
                except (KeyboardInterrupt, Exception) as e:
                    print(e)
                print('è¯­éŸ³è¯†åˆ«å·²ç»“æŸyyeyeyeyeyeye')
                UserInput.Speech.cancel_all_tasks()

        @staticmethod
        @socketio.on("audio_completed")
        def audio_completed():
            if UserInput.Speech.speaking:
                print('è¯­éŸ³è¯†åˆ«å·²ç»“æŸ')
                socketio.emit("toggle_button", {"button_id": "start-button", "state": False})
                UserInputAudio.start_shutdown_executor()
                UserInput.Speech.speaking = False

    # å¤„ç†å®¢æˆ·ç«¯æ–‡å­—è¯†åˆ«è¯·æ±‚
    @staticmethod
    @socketio.on("user_text")
    def user_text_loop(user_text):
        print(f"æ”¶åˆ°ç”¨æˆ·è¾“å…¥: {user_text}")
        user_text_html = markdown.markdown(user_text.replace('\n', '\n\n'), extensions=['fenced_code', 'extra'])
        socketio.emit("recognized_text", user_text_html)  # å‘é€ç”¨æˆ·è¾“å…¥åˆ°å‰ç«¯
        ChatSeries.chat_series[ChatSeries.current_chat_index].update_chat_history({"user": user_text_html})  # æ›´æ–°èŠå¤©è®°å½•

        UserInput.history.append(user_text)
        if len(UserInput.history) > 10:
            UserInput.history = UserInput.history[:10]
        text_history = '\n'.join(UserInput.history)
        memories = GlobalMemory.get_memories()
        if Config.use_RAG:
            retrieved_text = RAG.rag.get_retrieved_text(user_text)
            user_text_with_prompt = f'''<prompt>
ä»¥ä¸‹æ˜¯å¯¹è¯è®°å½•ï¼ˆæœ€åçš„æ˜¯æœ€æ–°ä¸€æ¬¡çš„å¯¹è¯è®°å½•ï¼‰ï¼š<history>
{text_history}
</history>
ä»¥ä¸‹æ˜¯å…¨å±€å¯¹è¯è®°å½•ï¼ˆåŒ…å«äº†æ‰€æœ‰å¯¹è¯ä¸­çš„æœ€è¿‘åæ¡æ¶ˆæ¯ï¼‰ï¼š<global_history>
{GlobalHistory.global_history._history}
</global_history>
è¿™æ˜¯å·²ç»æ·»åŠ çš„å…³äºç”¨æˆ·çš„ä¸€äº›å…¨å±€è®°å¿†ï¼š<global-memory>
{memories}
</global-memory>
ç”¨ç”¨æˆ·çš„é—®é¢˜å¯ä»¥æ£€ç´¢åˆ°ä»¥ä¸‹ç›¸å…³ä¿¡æ¯ï¼š
<rag>
{retrieved_text}
</rag>
ç”¨æˆ·çš„å¯¹è¯æœ€å¯èƒ½ä¸æœ€æ–°çš„ä¸€æ¬¡ï¼ˆå†å²ä¸­æœ€åä¸€ä¸ªï¼‰å¯¹è¯æœ‰å…³ã€‚
è¿™æ˜¯ç”¨æˆ·è¾“å…¥ï¼Œè¯·å›ç­”ï¼š<question>
{user_text}
</question>
ä½ çš„æ‰€æœ‰è¾“å‡ºæ‰€ç”¨çš„è¯­è¨€å¿…é¡»ä¸questionå†…çš„è¯­è¨€ä¸€è‡´ã€‚
</prompt>
'''
        else:
            user_text_with_prompt = f'''<prompt>
ä»¥ä¸‹æ˜¯å¯¹è¯è®°å½•ï¼ˆæœ€åçš„æ˜¯æœ€æ–°ä¸€æ¬¡çš„å¯¹è¯è®°å½•ï¼‰ï¼š<history>
{text_history}
</history>
ä»¥ä¸‹æ˜¯å…¨å±€å¯¹è¯è®°å½•ï¼ˆåŒ…å«äº†æ‰€æœ‰å¯¹è¯ä¸­çš„æœ€è¿‘åæ¡æ¶ˆæ¯ï¼‰ï¼š<global_history>
{GlobalHistory.global_history._history}
</global_history>
è¿™æ˜¯å·²ç»æ·»åŠ çš„å…³äºç”¨æˆ·çš„ä¸€äº›å…¨å±€è®°å¿†ï¼š<global-memory>
{memories}
</global-memory>
ç”¨æˆ·çš„å¯¹è¯æœ€å¯èƒ½ä¸æœ€æ–°çš„ä¸€æ¬¡ï¼ˆå†å²ä¸­æœ€åä¸€ä¸ªï¼‰å¯¹è¯æœ‰å…³ã€‚
è¿™æ˜¯ç”¨æˆ·è¾“å…¥ï¼Œè¯·å›ç­”ï¼š<question>
{user_text}
</question>
ä½ çš„æ‰€æœ‰è¾“å‡ºæ‰€ç”¨çš„è¯­è¨€å¿…é¡»ä¸questionå†…çš„è¯­è¨€ä¸€è‡´ã€‚
</prompt>
'''
        print(user_text_with_prompt)

        Bedrock.generate_response(user_text, user_text_with_prompt)  # ç”ŸæˆéŸ³é¢‘

    @staticmethod
    def update_history():
        current_chat = ChatSeries.chat_series[ChatSeries.current_chat_index]
        if current_chat.chat_history:
            for message in current_chat.chat_history:
                UserInput.history.append("ç”¨æˆ·ï¼š" + message["user"])
                try:
                    UserInput.history.append("æ¨¡å‹å›å¤ï¼š" + message["ai"])
                except KeyError:
                    UserInput.history.append("æ¨¡å‹å›å¤ï¼š" + "è¾“å‡ºè¢«æ‰“æ–­")
            if len(UserInput.history) > 20:
                UserInput.history = UserInput.history[-20:]
        else:
            UserInput.history = []

# è·¨å¯¹è¯è®°å¿†ç®¡ç†
class GlobalHistory:
    global_history = None

    """
    ä¸€ä¸ªç”¨äºLLMå¯¹è¯çš„å›ºå®šå¤§å°å†å²ç®¡ç†å™¨ï¼Œæ”¯æŒJSONæŒä¹…åŒ–ã€‚
    æœ€å¤šå­˜å‚¨ `max_size` æ¡é—®ç­”å¯¹ï¼Œå¹¶å°†å…¶ä¿å­˜åœ¨æœ¬åœ°æ–‡ä»¶ä¸­ã€‚
    """
    def __init__(self, file_path: Optional[str] = None, max_size: int = 10):
        self.max_size = max_size
        # ç¡®å®šJSONæ–‡ä»¶çš„è·¯å¾„
        self.file_path = Path(file_path or r'global_memory/GlobalHistory.json')
        # åŠ è½½å·²æœ‰å†å²æˆ–åˆå§‹åŒ–ä¸ºç©ºåˆ—è¡¨
        self._history: List[Tuple[str, str]] = self._load()

    def add_entry(self, question: str, response: str) -> None:
        """
        æ·»åŠ ä¸€æ¡æ–°çš„é—®ç­”å¯¹ï¼Œè‹¥è¶…å‡ºæœ€å¤§å®¹é‡åˆ™åˆ é™¤æœ€æ—©çš„æ¡ç›®ï¼Œ
        å¹¶ç«‹å³å°†æœ€æ–°å†å²æŒä¹…åŒ–åˆ°JSONæ–‡ä»¶ã€‚
        """
        self._history.append((question, response))
        # è‹¥è¶…è¿‡æœ€å¤§å®¹é‡ï¼Œåˆ™åˆ é™¤æœ€æ—©çš„å¤šä½™æ¡ç›®
        if len(self._history) > self.max_size:
            excess = len(self._history) - self.max_size
            self._history = self._history[excess:]
        self._save()

    def get_history(self) -> List[Tuple[str, str]]:#è¿”å›å½“å‰çš„å†å²åˆ—è¡¨
        return list(self._history)

    def clear(self) -> None:#æ¸…ç©ºå†…å­˜ä¸­çš„å†å²ï¼Œå¹¶åˆ é™¤JSONæ–‡ä»¶ã€‚

        self._history.clear()
        if self.file_path.exists():
            self.file_path.unlink()

    def __len__(self) -> int:
        return len(self._history)

    def __repr__(self) -> str:
        # è¿”å›å¯¹è±¡çš„å­—ç¬¦ä¸²è¡¨ç¤ºï¼ŒåŒ…å«å½“å‰å¤§å°å’Œæ–‡ä»¶è·¯å¾„
        return f"<HistoryManager size={len(self._history)}/{self.max_size} file='{self.file_path}'>"

    def _load(self) -> List[Tuple[str, str]]:
        """
        ä»JSONæ–‡ä»¶åŠ è½½å†å²ï¼Œè¿”å›é—®ç­”å…ƒç»„åˆ—è¡¨ã€‚
        å¦‚æœæ–‡ä»¶ä¸å­˜åœ¨æˆ–å†…å®¹æ— æ•ˆï¼Œåˆ™è¿”å›ç©ºåˆ—è¡¨ã€‚
        """
        if not self.file_path.exists():
            return []
        try:
            data = json.loads(self.file_path.read_text(encoding='utf-8'))
            # æœŸæœ›æ–‡ä»¶å†…å®¹ä¸ºåŒ…å« 'question' å’Œ 'response' çš„å­—å…¸åˆ—è¡¨
            history = [(item['question'], item['response']) for item in data]
            # è‹¥æ–‡ä»¶ä¸­è®°å½•è¶…è¿‡æœ€å¤§å®¹é‡ï¼Œåˆ™åªä¿ç•™æœ€æ–°çš„éƒ¨åˆ†
            return history[-self.max_size:]
        except (json.JSONDecodeError, KeyError, TypeError):
            # è¿”å›ç©ºåˆ—è¡¨ä»¥é˜²è§£æé”™è¯¯
            return []

    def _save(self) -> None:
        """
        å°†å½“å‰å†å²æŒä¹…åŒ–å†™å…¥JSONæ–‡ä»¶ã€‚
        """
        data = [{'question': q, 'response': r} for q, r in self._history]
        self.file_path.write_text(
            json.dumps(data, ensure_ascii=False, indent=2),
            encoding='utf-8'
        )

    @classmethod
    def load_from_file(cls, file_path: str, max_size: int = 10) -> 'GlobalHistory':
        """
        ä¾¿æ·æ„é€ æ–¹æ³•ï¼šæ ¹æ®å·²æœ‰æ–‡ä»¶è·¯å¾„åˆ›å»ºHistoryManagerå®ä¾‹ã€‚
        """
        return cls(file_path=file_path, max_size=max_size)

# å¤šæ¨¡æ€
class MultiModal:
    doc_list = []
    using_doc_list = []

    @staticmethod
    @socketio.on("clear_uploads")
    def clear_uploads():
        MultiModal.doc_list = []
        MultiModal.using_doc_list = []
        print('æ¸…ç©ºä¸Šä¼ åˆ—è¡¨')

        """
            æ¸…ç©º uploads ç›®å½•ä¸‹çš„æ‰€æœ‰æ–‡ä»¶å’Œå­ç›®å½•
            ä¿ç•™ uploads ç›®å½•æœ¬èº«
            """
        uploads_dir = Path('./uploads')

        try:
            # ğŸ›¡ï¸ æ£€æŸ¥ç›®å½•æ˜¯å¦å­˜åœ¨
            if not uploads_dir.exists():
                print(f"âš ï¸ ç›®å½•ä¸å­˜åœ¨: {uploads_dir}")
                return False

            # ğŸ—‘ï¸ éå†å¹¶åˆ é™¤æ‰€æœ‰å†…å®¹
            for item in uploads_dir.iterdir():
                try:
                    if item.is_file():
                        # ğŸ“ƒ åˆ é™¤æ–‡ä»¶
                        item.unlink()
                        print(f"âœ… å·²åˆ é™¤æ–‡ä»¶: {item.name}")
                    else:
                        # ğŸ“‚ åˆ é™¤å­ç›®å½•
                        shutil.rmtree(item)
                        print(f"âœ… å·²åˆ é™¤ç›®å½•: {item.name}")
                except Exception as e:
                    print(f"âŒ åˆ é™¤å¤±è´¥ {item.name}: {str(e)}")

            print(f"ğŸ‰ æˆåŠŸæ¸…ç©º {uploads_dir} ç›®å½•")
            return True

        except PermissionError:
            print(f"âŒ æƒé™ä¸è¶³ï¼Œæ— æ³•è®¿é—®ç›®å½•: {uploads_dir}")
        except Exception as e:
            print(f"âŒ æ“ä½œå¤±è´¥: {str(e)}")
        return False

    # å®‰å…¨æ ¡éªŒæ–‡ä»¶ç±»å‹
    @staticmethod
    def allowed_file(filename):
        return '.' in filename and \
            filename.rsplit('.', 1)[1].lower() in app.config['ALLOWED_EXTENSIONS']

    # ç”Ÿæˆå®‰å…¨æ–‡ä»¶å
    @staticmethod
    def secure_filename_with_timestamp(filename):
        timestamp = datetime.now().strftime("%Y%m%d%H%M%S")
        print(f"ç”Ÿæˆçš„æ–‡ä»¶åï¼š{timestamp}_{filename}")
        base_name = secure_filename(filename)
        return f"{timestamp}_{base_name}"

    # æ–‡ä»¶ä¸Šä¼ è·¯ç”±
    @staticmethod
    @app.route('/upload', methods=['POST'])
    def handle_upload():
        # æ£€æŸ¥è¯·æ±‚ä¸­æ˜¯å¦æœ‰æ–‡ä»¶
        if 'file' not in request.files:
            return jsonify(error="æ²¡æœ‰é€‰æ‹©æ–‡ä»¶"), 400

        file = request.files['file']

        # åŒé‡å®‰å…¨æ£€æŸ¥
        if file.filename == '':
            return jsonify(error="æ–‡ä»¶åä¸èƒ½ä¸ºç©º"), 400
        if not MultiModal.allowed_file(file.filename):
            return jsonify(error="ä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹"), 415

        try:
            # ğŸ”’ ç”Ÿæˆå®‰å…¨æ–‡ä»¶å
            safe_filename = MultiModal.secure_filename_with_timestamp(file.filename)

            # ğŸ“‚ è‡ªåŠ¨åˆ›å»ºä¸Šä¼ ç›®å½•
            save_path = os.path.join(app.config['UPLOAD_FOLDER'])
            os.makedirs(save_path, exist_ok=True)

            # ğŸ’¾ ä¿å­˜æ–‡ä»¶åˆ°æœ¬åœ°
            file.save(os.path.join(save_path, safe_filename))
            MultiModal.doc_list.append(safe_filename)
            MultiModal.using_doc_list.append(safe_filename)
            if len(MultiModal.using_doc_list) > 5:
                MultiModal.using_doc_list.pop(0)

            socketio.emit("upload_files", {"doc_list": MultiModal.doc_list, "using_doc_list": MultiModal.using_doc_list})

            return jsonify(
                success=True,
                message="æ–‡ä»¶ä¸Šä¼ æˆåŠŸï¼",
                filename=safe_filename,
                save_path=os.path.abspath(save_path)
            ), 200

        except Exception as e:
            return jsonify(error=f"ä¸Šä¼ å¤±è´¥: {str(e)}"), 500

    @staticmethod
    @socketio.on("remove_file")
    def remove_file(index):
        index = int(index)
        file_to_remove = MultiModal.doc_list[index]
        save_path = os.path.join(app.config['UPLOAD_FOLDER'], file_to_remove)
        os.makedirs(app.config['UPLOAD_FOLDER'], exist_ok=True)
        if os.path.exists(save_path):
            os.remove(save_path)
            print("æ–‡ä»¶å·²åˆ é™¤")

        MultiModal.doc_list.pop(index)
        if len(MultiModal.doc_list) >= 5:
            MultiModal.using_doc_list = MultiModal.doc_list[:5]
            print(MultiModal.using_doc_list)
        else:
            MultiModal.using_doc_list.pop(index)
            print(MultiModal.using_doc_list)

        socketio.emit("upload_files", {"doc_list": MultiModal.doc_list, "using_doc_list": MultiModal.using_doc_list})

    @staticmethod
    def decode_images():
        image_list = []
        using_image_list = [filename for filename in MultiModal.using_doc_list if filename[-4:] != '.pdf']

        for doc in using_image_list:
            with open(os.path.join(app.config['UPLOAD_FOLDER'], doc), 'rb') as f:
                image = dict()
                image['data'] = base64.b64encode(f.read()).decode("utf-8")
                image['type'] = "image/" + doc.split('.')[-1]
                image_list.append(image)

        return image_list

    @staticmethod
    def read_txt():
        txt_list = []
        using_txt_list = [filename for filename in MultiModal.using_doc_list if filename[-4:] == '.pdf']
        for doc in using_txt_list:
            file_path = os.path.join(app.config['UPLOAD_FOLDER'], doc)
            txt_list.append(RAG.process_pdf(file_path))
        return '\n'.join(txt_list)

# RAG
class RAG:
    doc_name = []
    doc_texts = []
    doc_indexes = [] # æ¯ä¸€ä¸ªæ–‡æ¡£æ–‡æœ¬ç»“æŸçš„ç´¢å¼•
    rag = None
    raw_pdf_list = []

    @staticmethod
    @socketio.on("RAG_refresh")
    def refresh_RAG():
        if Config.use_RAG and ((not RAG.rag) or (RAG.rag.embedding_model_name != Config.embedding_model_names[Config.embedding_model_name_index])):
            Config.use_RAG = False
            RAG.rag = RAG()
            print(Config.embedding_model_names[Config.embedding_model_name_index])
            print('RAGé‡æ–°åŠ è½½')
        elif Config.use_RAG:
            print('RAGå·²å°±ç»ª')
            socketio.emit("RAG_loaded")
        else:
            socketio.emit("RAG_unavailable")

    @staticmethod
    def get_embeddings(model, tokenizer, texts, batch_size=32):
        # æ‰¹é‡å¤„ç†é¿å…å†…å­˜æº¢å‡º
        embeddings = []
        for i in range(0, len(texts), batch_size):
            batch = texts[i:i + batch_size]

            inputs = tokenizer(
                batch,
                padding=True,
                truncation=True,
                return_tensors="pt",
                max_length=512
            )

            with torch.no_grad():
                outputs = model(**inputs)

            # ä½¿ç”¨æœ€åä¸€å±‚éšè—çŠ¶æ€çš„å‡å€¼ä½œä¸ºåµŒå…¥
            batch_embeddings = outputs.last_hidden_state.mean(dim=1)
            embeddings.append(batch_embeddings)

        return torch.cat(embeddings, dim=0).numpy().astype('float32')

    # åŠ è½½æ¨¡å‹
    def __init__(self):
        self.embedding_model_name = Config.embedding_model_names[Config.embedding_model_name_index]
        self.embedding_model = None
        self.embeddings = None

        def load_embeddings():
            try:
                socketio.emit("RAG_loading")
                import os
                os.environ["USE_FLASH_ATTENTION"] = "0"
                print(f"Loading {self.embedding_model_name}")
                if self.embedding_model_name == "Alibaba-NLP/get-multilingual-base":
                    self.embedding_model = AutoModel.from_pretrained("Alibaba-NLP/gte-multilingual-base")
                    self.tokenizer = AutoTokenizer.from_pretrained("Alibaba-NLP/gte-multilingual-base")

                    self.embeddings = RAG.get_embeddings(self.embedding_model, self.tokenizer, RAG.doc_texts)
                else:
                    self.embedding_model = SentenceTransformer(self.embedding_model_name, device="cuda", trust_remote_code=True)
                    self.embeddings = self.embedding_model.encode(RAG.doc_texts, normalize_embeddings=True, batch_size=32)
                print("Embeddings loaded successfully.")
                Config.use_RAG = True
                socketio.emit("RAG_loaded")
            except Exception as e:
                print(f"Error loading embeddings: {e}")
                socketio.emit("RAG_loading_failed", str(e).split(":")[0])

        embedding_thread = threading.Thread(target=load_embeddings)
        embedding_thread.daemon = True  # è®¾ç½®ä¸ºå®ˆæŠ¤çº¿ç¨‹ï¼ˆä¸»çº¿ç¨‹é€€å‡ºæ—¶è‡ªåŠ¨å…³é—­ï¼‰
        embedding_thread.start()

    # æ–‡æœ¬åˆå§‹åŒ–
    @staticmethod
    def text_init():
        folder = Path(r"RAG/processed_txt/")  # æ›¿æ¢å®é™…è·¯å¾„
        embed_model = None
        semantic_chunker = None
        for f in folder.iterdir():
            if not f.is_file():
                continue
            txt = ''
            output_file = os.path.join(r"RAG\chunked_txt", f.name)
            if os.path.exists(output_file):
                print(f"{output_file} å·²å­˜åœ¨ï¼Œè·³è¿‡åˆ†å—å¤„ç†ã€‚")
                RAG.doc_name.append(f.name)
                with open(output_file, "r", encoding='utf-8') as file:
                    txt = file.read()
                    RAG.doc_texts += txt.split('bubu')
                continue
            else:
                if not embed_model:
                    embed_model = HuggingFaceEmbeddings(
                        model_name="BAAI/bge-base-en-v1.5",
                        model_kwargs={'device': 'cuda'},  # æŒ‡å®šä½¿ç”¨CUDA
                        encode_kwargs={'normalize_embeddings': True}
                    )
                if not semantic_chunker:
                    semantic_chunker = SemanticChunker(
                        embeddings=embed_model,
                        breakpoint_threshold_type="interquartile"
                    )
                RAG.doc_name.append(f.name)
                with open(r"RAG/processed_txt/" + f.name, 'r', encoding='utf-8') as file:
                    loader = TextLoader(r"RAG/processed_txt/" + f.name, encoding="utf-8")
                    documents = loader.load()
                    texts = [doc.page_content for doc in documents]
                    semantic_chunks = semantic_chunker.create_documents(texts)  # âœ… ä¼ å…¥çº¯æ–‡æœ¬åˆ—è¡¨
                    for chunk in semantic_chunks:
                        RAG.doc_texts.append(chunk.page_content)
                        txt += chunk.page_content + 'bubu'
                with open(output_file, 'w', encoding='utf-8') as file:
                    file.write(txt) # ä¿å­˜åˆ†å—åçš„æ–‡æœ¬
                print(f"{output_file} å·²å®Œæˆåˆ†å—å¤„ç†ã€‚")
            RAG.doc_indexes.append(len(RAG.doc_texts) - 1)

    @staticmethod
    def process_pdf(filename):

        # æ·»åŠ ç¯å¢ƒå˜é‡
        import os
        poppler_path = r'./pdf_dependencies/poppler-24.08.0/Library/bin'
        os.environ["PATH"] += os.pathsep + poppler_path
        pytesseract_path = r'./pdf_dependencies/Tesseract-OCR'
        os.environ["PATH"] += os.pathsep + pytesseract_path
        from unstructured.partition.pdf import partition_pdf

        elements = partition_pdf(
            filename=filename,
            infer_table_structure=True,  # infer_table_structure=True è‡ªåŠ¨é€‰æ‹© hi_res ç­–ç•¥
            include_page_breaks=True
        )

        # æŒ‰ PageBreak åˆ†é¡µ
        all_sublists = []
        temp = []
        for el in elements:
            if el.category == "PageBreak":
                if temp:
                    all_sublists.append(temp)
                    temp = []
            else:
                temp.append(el)
        if temp:
            all_sublists.append(temp)

        pdf_elements = []
        # å¤„ç†æ¯ä¸€é¡µ
        for sublist in all_sublists:
            # æå–æ‰€æœ‰å…ƒç´ çš„åæ ‡
            coordinates = [element.metadata.coordinates.to_dict() for element in sublist]
            all_points = [coordinate['points'] for coordinate in coordinates]

            # è®¡ç®—ä¸­çº¿
            top_left_min = min(points[0][0] for points in all_points)  # æœ€å°çš„ x åæ ‡
            bottom_right_max = max(points[2][0] for points in all_points)  # æœ€å¤§çš„ x åæ ‡
            mid_line_x_coordinate = (top_left_min + bottom_right_max) / 2
            print(f"æœ€å°æ¨ªåæ ‡ï¼š{top_left_min}, æœ€å¤§æ¨ªåæ ‡ï¼š{bottom_right_max}, ä¸­çº¿ï¼š{mid_line_x_coordinate}")

            # åˆ†æ 
            left_column = []
            right_column = []
            for element, points in zip(sublist, all_points):
                top_left = min(points, key=lambda p: (p[0], p[1]))  # å·¦ä¸Šè§’ç‚¹
                if top_left[0] < mid_line_x_coordinate:
                    left_column.append(element)
                else:
                    right_column.append(element)

            # æŒ‰ y åæ ‡æ’åº
            left_column_sorted = sorted(left_column, key=lambda element:
            min(element.metadata.coordinates.to_dict()['points'], key=lambda p: (p[0], p[1]))[1])
            right_column_sorted = sorted(right_column, key=lambda element:
            min(element.metadata.coordinates.to_dict()['points'], key=lambda p: (p[0], p[1]))[1])

            # æ‰“å°ç»“æœ
            print("\nå·¦æ å…ƒç´ ï¼š")
            for element in left_column_sorted:
                print(element)

            print("\nå³æ å…ƒç´ ï¼š")
            for element in right_column_sorted:
                print(element)

            print("\n" + "=" * 50 + "\n")  # æ·»åŠ åˆ†éš”ç¬¦

            page_elements = [str(element) for element in left_column_sorted] + [str(element) for element in right_column_sorted]
            s_page_elements = '\n'.join(page_elements)
            pdf_elements.append(s_page_elements)

        return '\n'.join(pdf_elements)

    @staticmethod
    def process_pdf_for_rag():
        folder = Path(r"RAG/raw_pdf/")
        for f in folder.iterdir():
            if not f.is_file():
                continue
            RAG.raw_pdf_list.append(f.name)

        import os

        for file_name in RAG.raw_pdf_list:
            input_file = os.path.join("RAG/raw_pdf", file_name)
            output_file = os.path.join("RAG/processed_txt", os.path.splitext(file_name)[0] + ".txt")

            # åˆ¤æ–­è¾“å‡ºæ–‡ä»¶æ˜¯å¦å·²å­˜åœ¨
            if os.path.exists(output_file):
                print(f"{output_file} å·²å­˜åœ¨ï¼Œè·³è¿‡å¤„ç†ã€‚")
            else:
                pdf_result = RAG.process_pdf(input_file)
                with open(output_file, "w", encoding="utf-8") as f:
                    f.write(pdf_result)
                print(f"{file_name} å·²å¤„ç†å®Œæ¯•ï¼Œç»“æœä¿å­˜åˆ° {output_file}ã€‚")

    # æ ¹æ®ç”¨æˆ·è¾“å…¥å¯»æ‰¾å…³è”
    def get_retrieved_text(self, input_text):
        retrieved_text = dict()
        max_indexes = []
        max_index = 0
        pieced = False
        retrieved_texts = []

        def find_article_name(index):
            for i in range(len(RAG.doc_indexes)):
                if index <= RAG.doc_indexes[i]:
                    return RAG.doc_name[i]
            return None

        if pieced:
            pieces = 3
            while (len(input_text) - 6) % pieces != 0:
                input_text += ' '
            input_text += ' '
            input_text_pieces = [input_text[pieces * i: pieces * (i + 1) + 6] for i in range((len(input_text) - 6) // pieces)]
            input_text_pieces.append(input_text)
            input_embeddings = [self.embedding_model.encode(input_text_piece, normalize_embeddings=True) for input_text_piece in input_text_pieces]
            scores_list = [self.embedding_model.similarity(input_embedding, self.embeddings) for input_embedding in input_embeddings]
            max_indexes = [int(np.argmax(scores)) for scores in scores_list]
            max_indexes = list(set(max_indexes))  # å»é‡

            for index in max_indexes:
                retrieved_text["article"] = find_article_name(index)
                retrieved_text["content"] = RAG.doc_texts[index]

                retrieved_texts.append(retrieved_text)

            return retrieved_texts


        else:

            input_text_iter = Bedrock.char(
                "ç”¨æˆ·æƒ³ä½¿ç”¨RAGæ£€ç´¢ä¿¡æ¯ã€‚è¯·ä½ ä½¿ç”¨HyDEæŠ€æœ¯ï¼Œç†è§£ç”¨æˆ·éœ€æ±‚å¹¶ç”Ÿæˆä¸€ä¸ªç†æƒ³çš„ç­”æ¡ˆï¼Œä½ çš„å›ç­”å°†ç”¨äºRAGæ£€ç´¢ï¼Œè¯·ä¸è¦æ·»æ²¹åŠ é†‹ã€‚ä»¥ä¸‹æ˜¯ç”¨æˆ·çš„é—®é¢˜ï¼š" + input_text)

            regenerated_rag_questions = ''
            for iter in input_text_iter:
                regenerated_rag_questions += iter
            regenerated_rag_questions += "ç”¨æˆ·é—®é¢˜ï¼š" + input_text + "aiç†è§£"
            print(f"ç”¨æˆ·æé—®+aié‡æ–°é˜è¿°ï¼š{regenerated_rag_questions}")

            if self.embedding_model_name == 'Alibaba-NLP/get-multilingual-base':
                input_embedding = np.array(self.embedding_model.encode([regenerated_rag_questions]), dtype='float32')
                scores = cosine_similarity(input_embedding, self.embeddings)[0]  # é™ç»´
            else:
                input_embedding = self.embedding_model.encode([regenerated_rag_questions], normalize_embeddings=True)
                scores = self.embedding_model.similarity(input_embedding, self.embeddings)

            # å°† scores è½¬æ¢ä¸ºä¸€ç»´æ•°ç»„
            scores = scores.squeeze()  # å¦‚æœ scores æ˜¯äºŒç»´å¼ é‡ï¼Œä¾‹å¦‚å½¢çŠ¶ä¸º (1, n)ï¼Œåˆ™å°†å…¶è½¬æ¢ä¸ºä¸€ç»´æ•°ç»„
            if scores.ndim != 1:
                raise ValueError("scores å¿…é¡»æ˜¯ä¸€ç»´æ•°ç»„")
            if len(scores) < 3:
                raise ValueError("scores ä¸­å…ƒç´ ä¸è¶³ï¼Œæ— æ³•è·å–å‰ä¸‰å")

            # è·å–å¾—åˆ†æœ€é«˜çš„å‰ä¸‰ä¸ªç´¢å¼•ï¼ˆä½¿ç”¨ PyTorch çš„ argsortï¼‰
            import torch
            top_3_indices = torch.argsort(scores, descending=True)[:5]

            retrieved_texts = []
            for max_index in top_3_indices:
                retrieved_text = {}
                retrieved_text["article"] = find_article_name(max_index)
                if 4 <= max_index < len(RAG.doc_texts) - 4:
                    retrieved_text["content"] = '\n'.join(RAG.doc_texts[max_index - 4: max_index + 4])
                elif max_index < 4 and max_index < len(RAG.doc_texts) - 4:
                    retrieved_text["content"] = '\n'.join(RAG.doc_texts[: max_index + 4])
                elif max_index > len(RAG.doc_texts) - 4 and max_index >= 4:
                    retrieved_text["content"] = '\n'.join(RAG.doc_texts[max_index - 4:])
                else:
                    retrieved_text["content"] = '\n'.join(RAG.doc_texts)
                retrieved_texts.append(retrieved_text)
            return retrieved_texts

# å…¨å±€è®°å¿†ç®¡ç†
class GlobalMemory:
    memories = []

    @staticmethod
    @socketio.on("global_memory_init")
    def memory_init():
        if not os.path.exists(Config.MEMORY_FILE):
            with open(Config.MEMORY_FILE, 'w', encoding='utf-8') as f:
                json.dump([], f)

        with open(Config.MEMORY_FILE, encoding='utf-8') as f:
            memory_data = json.load(f)
            GlobalMemory.memories = [item["memory"] for item in memory_data]

        print('GlobalMemory.memories:', GlobalMemory.memories)

        socketio.emit("global_memory_refresh", [{"index": i, "memory": memory} for i, memory in enumerate(GlobalMemory.memories)])

    @staticmethod
    @socketio.on("add_memory")
    def add_memory(memory):
        """å­˜å‚¨å•æ¡æ•°æ®"""
        with open(Config.MEMORY_FILE, 'r+', encoding='utf-8') as f:
            data = json.load(f)
            # å°†å­—ç¬¦ä¸²åŒ…è£…æˆå­—å…¸æ ¼å¼
            memory_entry = {
                "memory": memory  # è¿™é‡Œä¼šåˆ›å»ºä¸€ä¸ªåŒ…å«memoryé”®çš„å­—å…¸
            }
            data.append(memory_entry)  # ç°åœ¨å­˜å‚¨çš„æ˜¯å­—å…¸è€Œä¸æ˜¯å­—ç¬¦ä¸²
            f.seek(0)
            json.dump(data, f)
        GlobalMemory.memories.append(memory)
        socketio.emit("global_memory_refresh", [{"index": i, "memory": memory} for i, memory in enumerate(GlobalMemory.memories)])

    @staticmethod
    @socketio.on("remove_memory")
    def remove_memory(index):
        index = int(index)
        GlobalMemory.memories.pop(index)
        with open(Config.MEMORY_FILE, 'r+', encoding='utf-8') as f:
            data = json.load(f)
            data.pop(index)
            # å›å†™æ–‡ä»¶
            f.seek(0)
            json.dump(data, f, indent=2, ensure_ascii=False)
            f.truncate()
        socketio.emit("global_memory_refresh", [{"index": i, "memory": memory} for i, memory in enumerate(GlobalMemory.memories)])

    @staticmethod
    def get_memories():
        return '\n'.join(GlobalMemory.memories)

# å¤šèŠå¤©è®°å½•ç®¡ç†
class ChatSeries:
    chat_series = []
    current_chat_index = 0
    new_message = True

    @staticmethod
    def init_chat_series():
        ChatSeries.load_local_chat_history()
        ChatSeries.current_chat_index = len(ChatSeries.chat_series) - 1

    @staticmethod
    @socketio.on("chat_list_init")
    def chat_list_init():
        socketio.emit("refresh_chat_list",
                      [{"index": len(ChatSeries.chat_series) - i - 1,
                        "chat_history": chat.chat_history,
                        "stress": False}
                       for i, chat in enumerate(ChatSeries.chat_series[::-1])])

    # åˆå§‹åŠ è½½æ•°æ®
    @staticmethod
    def load_local_chat_history():
        with open(Config.SERIES_FILE, 'r', encoding='utf-8') as f:
            chat_series = json.load(f)

        for chat_content in chat_series:
            chat = ChatSeries(chat_content)
            ChatSeries.chat_series.append(chat)

    def __init__(self, chat_content=None):
        self.chat_history = []
        if chat_content:
            self.chat_history = chat_content

    @staticmethod
    @socketio.on("choose_chat")
    def choose_chat(index):
        ChatSeries.current_chat_index = int(index)
        socketio.emit("new_chat_window")
        if ChatSeries.current_chat_index == -1:
            socketio.emit("back_to_home")
            return
        else:
            current_chat = ChatSeries.chat_series[ChatSeries.current_chat_index]
            current_chat.refresh_chat_list()
            current_chat.load_history()

    def load_history(self):
        for message in self.chat_history:
            socketio.emit("recognized_text", message["user"])
            socketio.emit("begin_generate_response")
            if "stopReason" in message:
                socketio.emit("retrievedInfo", {
                    "stopReason": message['stopReason'],
                    "stopSequence": message['stopSequence'],
                    "outputTokens": message['outputTokens']
                })
            if "ai" in message:
                socketio.emit("refresh_ai_response", message["ai_html"])
                socketio.emit("ai_response_end", message["ai"])
            else:
                socketio.emit("refresh_ai_response", "è¾“å‡ºè¢«æ‰“æ–­")
                socketio.emit("ai_response_end", "è¾“å‡ºè¢«æ‰“æ–­")
        UserInput.update_history()

    @staticmethod
    @socketio.on("delete_chat")
    def delete_chat(index):
        index = int(index)
        if not index == ChatSeries.current_chat_index:
            ChatSeries.chat_series.pop(index)
            with open(Config.SERIES_FILE, 'w', encoding='utf-8') as f:
                json.dump([chat.chat_history for chat in ChatSeries.chat_series], f, ensure_ascii=False, indent=2)
            if ChatSeries.chat_series:
                ChatSeries.current_chat_index = 0
                current_chat = ChatSeries.chat_series[ChatSeries.current_chat_index]
                current_chat.refresh_chat_list()
            else:
                ChatSeries.current_chat_index = -1
        else:
            ChatSeries.chat_series.pop(index)
            if ChatSeries.chat_series:
                ChatSeries.current_chat_index = 0
            else:
                ChatSeries.current_chat_index = -1
            with open(Config.SERIES_FILE, 'w', encoding='utf-8') as f:
                json.dump([chat.chat_history for chat in ChatSeries.chat_series], f, ensure_ascii=False, indent=2)
            socketio.emit("refresh_chat_list",
                          [{"index": len(ChatSeries.chat_series) - i - 1,
                            "chat_history": chat.chat_history,
                            "stress": False}
                           for i, chat in enumerate(ChatSeries.chat_series[::-1])])
            socketio.emit("back_to_home")

    def update_chat_history(self, target_dict):
        if ChatSeries.new_message:
            self.chat_history.append(target_dict)
            ChatSeries.new_message = False
            print("user_input loaded")
        else:
            for key in target_dict:
                self.chat_history[-1][key] = target_dict[key]
                print(f"{key} loaded")
        with open(Config.SERIES_FILE, 'w', encoding='utf-8') as f:
            json.dump([chat.chat_history for chat in ChatSeries.chat_series], f, ensure_ascii=False, indent=2)
        if "ai" in target_dict:
            ChatSeries.new_message = True

    @staticmethod
    @socketio.on("new_chat")
    def create_new_chat():
        new_chat = ChatSeries()
        ChatSeries.chat_series.append(new_chat)
        with open(Config.SERIES_FILE, 'w', encoding='utf-8') as f:
            json.dump([chat.chat_history for chat in ChatSeries.chat_series], f, ensure_ascii=False, indent=2)
        ChatSeries.current_chat_index = len(ChatSeries.chat_series) - 1
        socketio.emit("new_chat_window")
        ChatSeries.chat_series[ChatSeries.current_chat_index].refresh_chat_list()
        UserInput.history = []

    def refresh_chat_list(self):
        socketio.emit("refresh_chat_list",
                      [{"index": len(ChatSeries.chat_series) - i - 1,
                        "chat_history": chat.chat_history,
                        "stress": True if (self == chat) else False}
                       for i, chat in enumerate(ChatSeries.chat_series[::-1])])

# ä¸»å‡½æ•°
if __name__ == "__main__":
    RAG.text_init()
    ChatSeries.init_chat_series()
    GlobalHistory.global_history = GlobalHistory(file_path=r'global_memory/GlobalHistory.json', max_size=10)
    GlobalHistory.load_from_file(r'global_memory/GlobalHistory.json')

    if Config.refresh_rag_documents:
        pdf_process_thread = threading.Thread(target=RAG.process_pdf_for_rag)
        pdf_process_thread.daemon = True  # è®¾ç½®ä¸ºå®ˆæŠ¤çº¿ç¨‹ï¼ˆä¸»çº¿ç¨‹é€€å‡ºæ—¶è‡ªåŠ¨å…³é—­ï¼‰
        pdf_process_thread.start()
        print('å¼€å§‹å¤„ç†pdf')

    socketio.run(app, debug=False, allow_unsafe_werkzeug=True)